{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-06T09:01:49.745527Z",
     "iopub.status.busy": "2025-05-06T09:01:49.744890Z",
     "iopub.status.idle": "2025-05-06T09:20:13.558088Z",
     "shell.execute_reply": "2025-05-06T09:20:13.557082Z",
     "shell.execute_reply.started": "2025-05-06T09:01:49.745500Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "PyTorch version: 2.5.1+cu124\n",
      "Timm version: 1.0.14\n",
      "Downloading dataset...\n",
      "Dataset downloaded to: /kaggle/input/adni-4c-alzheimers-mri-classification-dataset\n",
      "\n",
      "Collecting image paths and labels...\n",
      "--- Searching for Image Directory ---\n",
      "Checking: /kaggle/input/adni-4c-alzheimers-mri-classification-dataset/ADNI_4C_MRI_Classification_Dataset/AugmentedAlzheimerDataset\n",
      "  > Directory does not exist.\n",
      "Checking: /kaggle/input/adni-4c-alzheimers-mri-classification-dataset/AugmentedAlzheimerDataset\n",
      "Found valid image directory: /kaggle/input/adni-4c-alzheimers-mri-classification-dataset/AugmentedAlzheimerDataset\n",
      "--- Search Complete ---\n",
      "\n",
      "Using image directory: /kaggle/input/adni-4c-alzheimers-mri-classification-dataset/AugmentedAlzheimerDataset\n",
      "Found 6464 images in 'CN'\n",
      "Found 9600 images in 'EMCI'\n",
      "Found 8960 images in 'LMCI'\n",
      "Found 8960 images in 'AD'\n",
      "Total images found: 33984\n",
      "\n",
      "Class distribution:\n",
      "Class CN: 6464 images\n",
      "Class EMCI: 9600 images\n",
      "Class LMCI: 8960 images\n",
      "Class AD: 8960 images\n",
      "\n",
      "Training set size: 27187\n",
      "Validation set size: 6797\n",
      "DataLoaders created.\n",
      "Freezing MobileNetV2 backbone.\n",
      "Loading pre-trained timm model: deit_small_patch16_224\n",
      "Freezing DeiT blocks.\n",
      "\n",
      "--- Initial Trainable Parameters ---\n",
      "cls_token\n",
      "pos_embed\n",
      "projection.weight\n",
      "projection.bias\n",
      "deit_norm.weight\n",
      "deit_norm.bias\n",
      "classifier.weight\n",
      "classifier.bias\n",
      "\n",
      "--- Starting Phase 1: Training Head ---\n",
      "Epoch [1/15], Step [50/1700], Loss: 1.2740\n",
      "Epoch [1/15], Step [100/1700], Loss: 1.1628\n",
      "Epoch [1/15], Step [150/1700], Loss: 1.4971\n",
      "Epoch [1/15], Step [200/1700], Loss: 1.0127\n",
      "Epoch [1/15], Step [250/1700], Loss: 1.1483\n",
      "Epoch [1/15], Step [300/1700], Loss: 1.0264\n",
      "Epoch [1/15], Step [350/1700], Loss: 1.0950\n",
      "Epoch [1/15], Step [400/1700], Loss: 0.9603\n",
      "Epoch [1/15], Step [450/1700], Loss: 0.8831\n",
      "Epoch [1/15], Step [500/1700], Loss: 0.9912\n",
      "Epoch [1/15], Step [550/1700], Loss: 0.7510\n",
      "Epoch [1/15], Step [600/1700], Loss: 0.4940\n",
      "Epoch [1/15], Step [650/1700], Loss: 0.9601\n",
      "Epoch [1/15], Step [700/1700], Loss: 1.1036\n",
      "Epoch [1/15], Step [750/1700], Loss: 0.9069\n",
      "Epoch [1/15], Step [800/1700], Loss: 0.9119\n",
      "Epoch [1/15], Step [850/1700], Loss: 0.8288\n",
      "Epoch [1/15], Step [900/1700], Loss: 0.7375\n",
      "Epoch [1/15], Step [950/1700], Loss: 0.8205\n",
      "Epoch [1/15], Step [1000/1700], Loss: 0.4750\n",
      "Epoch [1/15], Step [1050/1700], Loss: 1.1869\n",
      "Epoch [1/15], Step [1100/1700], Loss: 0.8898\n",
      "Epoch [1/15], Step [1150/1700], Loss: 0.8408\n",
      "Epoch [1/15], Step [1200/1700], Loss: 0.5099\n",
      "Epoch [1/15], Step [1250/1700], Loss: 0.5795\n",
      "Epoch [1/15], Step [1300/1700], Loss: 0.8099\n",
      "Epoch [1/15], Step [1350/1700], Loss: 0.6659\n",
      "Epoch [1/15], Step [1400/1700], Loss: 0.8430\n",
      "Epoch [1/15], Step [1450/1700], Loss: 0.7930\n",
      "Epoch [1/15], Step [1500/1700], Loss: 0.7504\n",
      "Epoch [1/15], Step [1550/1700], Loss: 0.6955\n",
      "Epoch [1/15], Step [1600/1700], Loss: 0.6225\n",
      "Epoch [1/15], Step [1650/1700], Loss: 0.8644\n",
      "Epoch [1/15], Step [1700/1700], Loss: 0.7926\n",
      "Epoch 1/15 Summary | Train Loss: 0.9381 | Train Acc: 57.10%\n",
      "Epoch 1/15 Summary | Val Loss: 0.7477 | Val Acc: 66.65%\n",
      "Epoch [2/15], Step [50/1700], Loss: 0.6073\n",
      "Epoch [2/15], Step [100/1700], Loss: 0.7320\n",
      "Epoch [2/15], Step [150/1700], Loss: 0.5728\n",
      "Epoch [2/15], Step [200/1700], Loss: 0.7378\n",
      "Epoch [2/15], Step [250/1700], Loss: 0.5997\n",
      "Epoch [2/15], Step [300/1700], Loss: 0.5957\n",
      "Epoch [2/15], Step [350/1700], Loss: 0.5236\n",
      "Epoch [2/15], Step [400/1700], Loss: 0.8000\n",
      "Epoch [2/15], Step [450/1700], Loss: 0.8402\n",
      "Epoch [2/15], Step [500/1700], Loss: 0.6683\n",
      "Epoch [2/15], Step [550/1700], Loss: 0.9879\n",
      "Epoch [2/15], Step [600/1700], Loss: 0.9451\n",
      "Epoch [2/15], Step [650/1700], Loss: 0.9460\n",
      "Epoch [2/15], Step [700/1700], Loss: 0.4851\n",
      "Epoch [2/15], Step [750/1700], Loss: 0.8172\n",
      "Epoch [2/15], Step [800/1700], Loss: 0.6379\n",
      "Epoch [2/15], Step [850/1700], Loss: 0.6577\n",
      "Epoch [2/15], Step [900/1700], Loss: 0.5219\n",
      "Epoch [2/15], Step [950/1700], Loss: 0.4387\n",
      "Epoch [2/15], Step [1000/1700], Loss: 0.4952\n",
      "Epoch [2/15], Step [1050/1700], Loss: 0.3667\n",
      "Epoch [2/15], Step [1100/1700], Loss: 0.7471\n",
      "Epoch [2/15], Step [1150/1700], Loss: 0.7242\n",
      "Epoch [2/15], Step [1200/1700], Loss: 0.7745\n",
      "Epoch [2/15], Step [1250/1700], Loss: 0.3838\n",
      "Epoch [2/15], Step [1300/1700], Loss: 0.7431\n",
      "Epoch [2/15], Step [1350/1700], Loss: 0.8220\n",
      "Epoch [2/15], Step [1400/1700], Loss: 0.7804\n",
      "Epoch [2/15], Step [1450/1700], Loss: 0.8498\n",
      "Epoch [2/15], Step [1500/1700], Loss: 0.5210\n",
      "Epoch [2/15], Step [1550/1700], Loss: 0.6675\n",
      "Epoch [2/15], Step [1600/1700], Loss: 0.4830\n",
      "Epoch [2/15], Step [1650/1700], Loss: 0.8824\n",
      "Epoch [2/15], Step [1700/1700], Loss: 1.1165\n",
      "Epoch 2/15 Summary | Train Loss: 0.7158 | Train Acc: 68.10%\n",
      "Epoch 2/15 Summary | Val Loss: 0.6752 | Val Acc: 70.15%\n",
      "Epoch [3/15], Step [50/1700], Loss: 0.4998\n",
      "Epoch [3/15], Step [100/1700], Loss: 0.4752\n",
      "Epoch [3/15], Step [150/1700], Loss: 0.8249\n",
      "Epoch [3/15], Step [200/1700], Loss: 0.8062\n",
      "Epoch [3/15], Step [250/1700], Loss: 0.2632\n",
      "Epoch [3/15], Step [300/1700], Loss: 0.6905\n",
      "Epoch [3/15], Step [350/1700], Loss: 0.8633\n",
      "Epoch [3/15], Step [400/1700], Loss: 0.5625\n",
      "Epoch [3/15], Step [450/1700], Loss: 0.4783\n",
      "Epoch [3/15], Step [500/1700], Loss: 0.4583\n",
      "Epoch [3/15], Step [550/1700], Loss: 0.4046\n",
      "Epoch [3/15], Step [600/1700], Loss: 0.4769\n",
      "Epoch [3/15], Step [650/1700], Loss: 0.5224\n",
      "Epoch [3/15], Step [700/1700], Loss: 0.5924\n",
      "Epoch [3/15], Step [750/1700], Loss: 0.8188\n",
      "Epoch [3/15], Step [800/1700], Loss: 0.5697\n",
      "Epoch [3/15], Step [850/1700], Loss: 0.5474\n",
      "Epoch [3/15], Step [900/1700], Loss: 0.4890\n",
      "Epoch [3/15], Step [950/1700], Loss: 0.6306\n",
      "Epoch [3/15], Step [1000/1700], Loss: 0.5196\n",
      "Epoch [3/15], Step [1050/1700], Loss: 0.8883\n",
      "Epoch [3/15], Step [1100/1700], Loss: 0.5823\n",
      "Epoch [3/15], Step [1150/1700], Loss: 1.1477\n",
      "Epoch [3/15], Step [1200/1700], Loss: 0.7738\n",
      "Epoch [3/15], Step [1250/1700], Loss: 0.6186\n",
      "Epoch [3/15], Step [1300/1700], Loss: 0.6659\n",
      "Epoch [3/15], Step [1350/1700], Loss: 0.9693\n",
      "Epoch [3/15], Step [1400/1700], Loss: 0.6661\n",
      "Epoch [3/15], Step [1450/1700], Loss: 0.5908\n",
      "Epoch [3/15], Step [1500/1700], Loss: 0.3376\n",
      "Epoch [3/15], Step [1550/1700], Loss: 0.6747\n",
      "Epoch [3/15], Step [1600/1700], Loss: 0.2859\n",
      "Epoch [3/15], Step [1650/1700], Loss: 0.4650\n",
      "Epoch [3/15], Step [1700/1700], Loss: 1.0332\n",
      "Epoch 3/15 Summary | Train Loss: 0.6066 | Train Acc: 73.71%\n",
      "Epoch 3/15 Summary | Val Loss: 0.5825 | Val Acc: 74.67%\n",
      "Epoch [4/15], Step [50/1700], Loss: 0.4360\n",
      "Epoch [4/15], Step [100/1700], Loss: 0.5104\n",
      "Epoch [4/15], Step [150/1700], Loss: 0.2084\n",
      "Epoch [4/15], Step [200/1700], Loss: 0.8034\n",
      "Epoch [4/15], Step [250/1700], Loss: 0.6208\n",
      "Epoch [4/15], Step [300/1700], Loss: 0.3998\n",
      "Epoch [4/15], Step [350/1700], Loss: 0.7640\n",
      "Epoch [4/15], Step [400/1700], Loss: 0.5851\n",
      "Epoch [4/15], Step [450/1700], Loss: 0.3459\n",
      "Epoch [4/15], Step [500/1700], Loss: 0.4785\n",
      "Epoch [4/15], Step [550/1700], Loss: 0.6242\n",
      "Epoch [4/15], Step [600/1700], Loss: 0.6812\n",
      "Epoch [4/15], Step [650/1700], Loss: 0.2062\n",
      "Epoch [4/15], Step [700/1700], Loss: 0.8179\n",
      "Epoch [4/15], Step [750/1700], Loss: 0.3833\n",
      "Epoch [4/15], Step [800/1700], Loss: 0.5011\n",
      "Epoch [4/15], Step [850/1700], Loss: 0.5077\n",
      "Epoch [4/15], Step [900/1700], Loss: 0.6302\n",
      "Epoch [4/15], Step [950/1700], Loss: 0.6496\n",
      "Epoch [4/15], Step [1000/1700], Loss: 0.3319\n",
      "Epoch [4/15], Step [1050/1700], Loss: 0.3097\n",
      "Epoch [4/15], Step [1100/1700], Loss: 0.5074\n",
      "Epoch [4/15], Step [1150/1700], Loss: 0.5164\n",
      "Epoch [4/15], Step [1200/1700], Loss: 0.9297\n",
      "Epoch [4/15], Step [1250/1700], Loss: 0.4312\n",
      "Epoch [4/15], Step [1300/1700], Loss: 0.7355\n",
      "Epoch [4/15], Step [1350/1700], Loss: 0.4857\n",
      "Epoch [4/15], Step [1400/1700], Loss: 0.6978\n",
      "Epoch [4/15], Step [1450/1700], Loss: 0.8338\n",
      "Epoch [4/15], Step [1500/1700], Loss: 0.8637\n",
      "Epoch [4/15], Step [1550/1700], Loss: 0.5916\n",
      "Epoch [4/15], Step [1600/1700], Loss: 0.2259\n",
      "Epoch [4/15], Step [1650/1700], Loss: 0.4194\n",
      "Epoch [4/15], Step [1700/1700], Loss: 0.6639\n",
      "Epoch 4/15 Summary | Train Loss: 0.5377 | Train Acc: 77.31%\n",
      "Epoch 4/15 Summary | Val Loss: 0.5299 | Val Acc: 76.96%\n",
      "Epoch [5/15], Step [50/1700], Loss: 0.2712\n",
      "Epoch [5/15], Step [100/1700], Loss: 0.6500\n",
      "Epoch [5/15], Step [150/1700], Loss: 0.4616\n",
      "Epoch [5/15], Step [200/1700], Loss: 0.3252\n",
      "Epoch [5/15], Step [250/1700], Loss: 0.3925\n",
      "Epoch [5/15], Step [300/1700], Loss: 0.2875\n",
      "Epoch [5/15], Step [350/1700], Loss: 0.6087\n",
      "Epoch [5/15], Step [400/1700], Loss: 0.4945\n",
      "Epoch [5/15], Step [450/1700], Loss: 0.2879\n",
      "Epoch [5/15], Step [500/1700], Loss: 0.4711\n",
      "Epoch [5/15], Step [550/1700], Loss: 0.1785\n",
      "Epoch [5/15], Step [600/1700], Loss: 0.5464\n",
      "Epoch [5/15], Step [650/1700], Loss: 0.4925\n",
      "Epoch [5/15], Step [700/1700], Loss: 0.3162\n",
      "Epoch [5/15], Step [750/1700], Loss: 0.7039\n",
      "Epoch [5/15], Step [800/1700], Loss: 0.2985\n",
      "Epoch [5/15], Step [850/1700], Loss: 0.3803\n",
      "Epoch [5/15], Step [900/1700], Loss: 0.3691\n",
      "Epoch [5/15], Step [950/1700], Loss: 0.6103\n",
      "Epoch [5/15], Step [1000/1700], Loss: 0.7105\n",
      "Epoch [5/15], Step [1050/1700], Loss: 0.6025\n",
      "Epoch [5/15], Step [1100/1700], Loss: 0.3786\n",
      "Epoch [5/15], Step [1150/1700], Loss: 0.6451\n",
      "Epoch [5/15], Step [1200/1700], Loss: 0.4320\n",
      "Epoch [5/15], Step [1250/1700], Loss: 0.2562\n",
      "Epoch [5/15], Step [1300/1700], Loss: 0.3475\n",
      "Epoch [5/15], Step [1350/1700], Loss: 0.3909\n",
      "Epoch [5/15], Step [1400/1700], Loss: 0.2491\n",
      "Epoch [5/15], Step [1450/1700], Loss: 0.4297\n",
      "Epoch [5/15], Step [1500/1700], Loss: 0.2370\n",
      "Epoch [5/15], Step [1550/1700], Loss: 0.5797\n",
      "Epoch [5/15], Step [1600/1700], Loss: 0.2530\n",
      "Epoch [5/15], Step [1650/1700], Loss: 0.2837\n",
      "Epoch [5/15], Step [1700/1700], Loss: 0.5525\n",
      "Epoch 5/15 Summary | Train Loss: 0.4726 | Train Acc: 80.04%\n",
      "Epoch 5/15 Summary | Val Loss: 0.5109 | Val Acc: 78.24%\n",
      "Epoch [6/15], Step [50/1700], Loss: 0.2831\n",
      "Epoch [6/15], Step [100/1700], Loss: 0.3764\n",
      "Epoch [6/15], Step [150/1700], Loss: 0.4207\n",
      "Epoch [6/15], Step [200/1700], Loss: 0.4426\n",
      "Epoch [6/15], Step [250/1700], Loss: 0.1881\n",
      "Epoch [6/15], Step [300/1700], Loss: 0.6419\n",
      "Epoch [6/15], Step [350/1700], Loss: 0.5229\n",
      "Epoch [6/15], Step [400/1700], Loss: 0.4705\n",
      "Epoch [6/15], Step [450/1700], Loss: 0.3295\n",
      "Epoch [6/15], Step [500/1700], Loss: 0.4868\n",
      "Epoch [6/15], Step [550/1700], Loss: 0.3899\n",
      "Epoch [6/15], Step [600/1700], Loss: 0.6521\n",
      "Epoch [6/15], Step [650/1700], Loss: 0.3660\n",
      "Epoch [6/15], Step [700/1700], Loss: 0.3374\n",
      "Epoch [6/15], Step [750/1700], Loss: 0.5131\n",
      "Epoch [6/15], Step [800/1700], Loss: 0.3347\n",
      "Epoch [6/15], Step [850/1700], Loss: 0.6025\n",
      "Epoch [6/15], Step [900/1700], Loss: 0.2579\n",
      "Epoch [6/15], Step [950/1700], Loss: 0.1449\n",
      "Epoch [6/15], Step [1000/1700], Loss: 0.3659\n",
      "Epoch [6/15], Step [1050/1700], Loss: 0.5548\n",
      "Epoch [6/15], Step [1100/1700], Loss: 0.7969\n",
      "Epoch [6/15], Step [1150/1700], Loss: 0.4196\n",
      "Epoch [6/15], Step [1200/1700], Loss: 0.5268\n",
      "Epoch [6/15], Step [1250/1700], Loss: 0.6579\n",
      "Epoch [6/15], Step [1300/1700], Loss: 0.3568\n",
      "Epoch [6/15], Step [1350/1700], Loss: 0.2915\n",
      "Epoch [6/15], Step [1400/1700], Loss: 0.2471\n",
      "Epoch [6/15], Step [1450/1700], Loss: 0.6826\n",
      "Epoch [6/15], Step [1500/1700], Loss: 0.5240\n",
      "Epoch [6/15], Step [1550/1700], Loss: 0.7919\n",
      "Epoch [6/15], Step [1600/1700], Loss: 0.6758\n",
      "Epoch [6/15], Step [1650/1700], Loss: 0.4660\n",
      "Epoch [6/15], Step [1700/1700], Loss: 0.1466\n",
      "Epoch 6/15 Summary | Train Loss: 0.4203 | Train Acc: 82.81%\n",
      "Epoch 6/15 Summary | Val Loss: 0.4773 | Val Acc: 79.90%\n",
      "Epoch [7/15], Step [50/1700], Loss: 0.4084\n",
      "Epoch [7/15], Step [100/1700], Loss: 0.4008\n",
      "Epoch [7/15], Step [150/1700], Loss: 0.2015\n",
      "Epoch [7/15], Step [200/1700], Loss: 0.4290\n",
      "Epoch [7/15], Step [250/1700], Loss: 0.1840\n",
      "Epoch [7/15], Step [300/1700], Loss: 0.3573\n",
      "Epoch [7/15], Step [350/1700], Loss: 0.3232\n",
      "Epoch [7/15], Step [400/1700], Loss: 0.2254\n",
      "Epoch [7/15], Step [450/1700], Loss: 0.3873\n",
      "Epoch [7/15], Step [500/1700], Loss: 0.5449\n",
      "Epoch [7/15], Step [550/1700], Loss: 0.3379\n",
      "Epoch [7/15], Step [600/1700], Loss: 0.4819\n",
      "Epoch [7/15], Step [650/1700], Loss: 0.2661\n",
      "Epoch [7/15], Step [700/1700], Loss: 0.2550\n",
      "Epoch [7/15], Step [750/1700], Loss: 0.2473\n",
      "Epoch [7/15], Step [800/1700], Loss: 0.4676\n",
      "Epoch [7/15], Step [850/1700], Loss: 0.1332\n",
      "Epoch [7/15], Step [900/1700], Loss: 0.3572\n",
      "Epoch [7/15], Step [950/1700], Loss: 0.2052\n",
      "Epoch [7/15], Step [1000/1700], Loss: 0.3677\n",
      "Epoch [7/15], Step [1050/1700], Loss: 0.6043\n",
      "Epoch [7/15], Step [1100/1700], Loss: 0.6812\n",
      "Epoch [7/15], Step [1150/1700], Loss: 0.3723\n",
      "Epoch [7/15], Step [1200/1700], Loss: 0.2293\n",
      "Epoch [7/15], Step [1250/1700], Loss: 0.3530\n",
      "Epoch [7/15], Step [1300/1700], Loss: 0.4867\n",
      "Epoch [7/15], Step [1350/1700], Loss: 0.2573\n",
      "Epoch [7/15], Step [1400/1700], Loss: 0.7964\n",
      "Epoch [7/15], Step [1450/1700], Loss: 0.2338\n",
      "Epoch [7/15], Step [1500/1700], Loss: 0.2287\n",
      "Epoch [7/15], Step [1550/1700], Loss: 0.4397\n",
      "Epoch [7/15], Step [1600/1700], Loss: 0.1767\n",
      "Epoch [7/15], Step [1650/1700], Loss: 0.3192\n",
      "Epoch [7/15], Step [1700/1700], Loss: 0.8249\n",
      "Epoch 7/15 Summary | Train Loss: 0.3780 | Train Acc: 84.83%\n",
      "Epoch 7/15 Summary | Val Loss: 0.4957 | Val Acc: 78.95%\n",
      "Epoch [8/15], Step [50/1700], Loss: 0.1396\n",
      "Epoch [8/15], Step [100/1700], Loss: 0.2630\n",
      "Epoch [8/15], Step [150/1700], Loss: 0.5371\n",
      "Epoch [8/15], Step [200/1700], Loss: 0.2955\n",
      "Epoch [8/15], Step [250/1700], Loss: 0.3251\n",
      "Epoch [8/15], Step [300/1700], Loss: 0.2831\n",
      "Epoch [8/15], Step [350/1700], Loss: 0.3297\n",
      "Epoch [8/15], Step [400/1700], Loss: 0.4294\n",
      "Epoch [8/15], Step [450/1700], Loss: 0.2086\n",
      "Epoch [8/15], Step [500/1700], Loss: 0.1052\n",
      "Epoch [8/15], Step [550/1700], Loss: 0.2932\n",
      "Epoch [8/15], Step [600/1700], Loss: 0.5040\n",
      "Epoch [8/15], Step [650/1700], Loss: 0.2855\n",
      "Epoch [8/15], Step [700/1700], Loss: 0.3877\n",
      "Epoch [8/15], Step [750/1700], Loss: 0.1382\n",
      "Epoch [8/15], Step [800/1700], Loss: 0.3765\n",
      "Epoch [8/15], Step [850/1700], Loss: 0.2754\n",
      "Epoch [8/15], Step [900/1700], Loss: 0.2079\n",
      "Epoch [8/15], Step [950/1700], Loss: 0.4720\n",
      "Epoch [8/15], Step [1000/1700], Loss: 0.5507\n",
      "Epoch [8/15], Step [1050/1700], Loss: 0.3736\n",
      "Epoch [8/15], Step [1100/1700], Loss: 0.1966\n",
      "Epoch [8/15], Step [1150/1700], Loss: 0.2921\n",
      "Epoch [8/15], Step [1200/1700], Loss: 0.4165\n",
      "Epoch [8/15], Step [1250/1700], Loss: 0.3603\n",
      "Epoch [8/15], Step [1300/1700], Loss: 0.3432\n",
      "Epoch [8/15], Step [1350/1700], Loss: 0.4431\n",
      "Epoch [8/15], Step [1400/1700], Loss: 0.3874\n",
      "Epoch [8/15], Step [1450/1700], Loss: 0.2925\n",
      "Epoch [8/15], Step [1500/1700], Loss: 0.5459\n",
      "Epoch [8/15], Step [1550/1700], Loss: 0.4147\n",
      "Epoch [8/15], Step [1600/1700], Loss: 0.6673\n",
      "Epoch [8/15], Step [1650/1700], Loss: 0.4401\n",
      "Epoch [8/15], Step [1700/1700], Loss: 0.3001\n",
      "Epoch 8/15 Summary | Train Loss: 0.3339 | Train Acc: 86.63%\n",
      "Epoch 8/15 Summary | Val Loss: 0.4540 | Val Acc: 81.64%\n",
      "Epoch [9/15], Step [50/1700], Loss: 0.2698\n",
      "Epoch [9/15], Step [100/1700], Loss: 0.3230\n",
      "Epoch [9/15], Step [150/1700], Loss: 0.3745\n",
      "Epoch [9/15], Step [200/1700], Loss: 0.3413\n",
      "Epoch [9/15], Step [250/1700], Loss: 0.7042\n",
      "Epoch [9/15], Step [300/1700], Loss: 0.2151\n",
      "Epoch [9/15], Step [350/1700], Loss: 0.1334\n",
      "Epoch [9/15], Step [400/1700], Loss: 0.2695\n",
      "Epoch [9/15], Step [450/1700], Loss: 0.2800\n",
      "Epoch [9/15], Step [500/1700], Loss: 0.3893\n",
      "Epoch [9/15], Step [550/1700], Loss: 0.3651\n",
      "Epoch [9/15], Step [600/1700], Loss: 0.3558\n",
      "Epoch [9/15], Step [650/1700], Loss: 0.1790\n",
      "Epoch [9/15], Step [700/1700], Loss: 0.1985\n",
      "Epoch [9/15], Step [750/1700], Loss: 0.2112\n",
      "Epoch [9/15], Step [800/1700], Loss: 0.4793\n",
      "Epoch [9/15], Step [850/1700], Loss: 0.5098\n",
      "Epoch [9/15], Step [900/1700], Loss: 0.2343\n",
      "Epoch [9/15], Step [950/1700], Loss: 0.4389\n",
      "Epoch [9/15], Step [1000/1700], Loss: 0.3896\n",
      "Epoch [9/15], Step [1050/1700], Loss: 0.4353\n",
      "Epoch [9/15], Step [1100/1700], Loss: 0.5948\n",
      "Epoch [9/15], Step [1150/1700], Loss: 0.8057\n",
      "Epoch [9/15], Step [1200/1700], Loss: 0.2424\n",
      "Epoch [9/15], Step [1250/1700], Loss: 0.1358\n",
      "Epoch [9/15], Step [1300/1700], Loss: 0.3973\n",
      "Epoch [9/15], Step [1350/1700], Loss: 0.1711\n",
      "Epoch [9/15], Step [1400/1700], Loss: 0.1309\n",
      "Epoch [9/15], Step [1450/1700], Loss: 0.4274\n",
      "Epoch [9/15], Step [1500/1700], Loss: 0.3241\n",
      "Epoch [9/15], Step [1550/1700], Loss: 0.4551\n",
      "Epoch [9/15], Step [1600/1700], Loss: 0.4392\n",
      "Epoch [9/15], Step [1650/1700], Loss: 0.2373\n",
      "Epoch [9/15], Step [1700/1700], Loss: 0.3247\n",
      "Epoch 9/15 Summary | Train Loss: 0.3129 | Train Acc: 87.60%\n",
      "Epoch 9/15 Summary | Val Loss: 0.4444 | Val Acc: 81.99%\n",
      "Epoch [10/15], Step [50/1700], Loss: 0.1798\n",
      "Epoch [10/15], Step [100/1700], Loss: 0.3796\n",
      "Epoch [10/15], Step [150/1700], Loss: 0.3962\n",
      "Epoch [10/15], Step [200/1700], Loss: 0.1782\n",
      "Epoch [10/15], Step [250/1700], Loss: 0.2165\n",
      "Epoch [10/15], Step [300/1700], Loss: 0.1184\n",
      "Epoch [10/15], Step [350/1700], Loss: 0.2356\n",
      "Epoch [10/15], Step [400/1700], Loss: 0.2236\n",
      "Epoch [10/15], Step [450/1700], Loss: 0.2989\n",
      "Epoch [10/15], Step [500/1700], Loss: 0.2208\n",
      "Epoch [10/15], Step [550/1700], Loss: 0.2633\n",
      "Epoch [10/15], Step [600/1700], Loss: 0.0684\n",
      "Epoch [10/15], Step [650/1700], Loss: 0.2526\n",
      "Epoch [10/15], Step [700/1700], Loss: 0.1709\n",
      "Epoch [10/15], Step [750/1700], Loss: 0.6421\n",
      "Epoch [10/15], Step [800/1700], Loss: 0.4096\n",
      "Epoch [10/15], Step [850/1700], Loss: 0.1334\n",
      "Epoch [10/15], Step [900/1700], Loss: 0.2720\n",
      "Epoch [10/15], Step [950/1700], Loss: 0.3365\n",
      "Epoch [10/15], Step [1000/1700], Loss: 0.3211\n",
      "Epoch [10/15], Step [1050/1700], Loss: 0.1624\n",
      "Epoch [10/15], Step [1100/1700], Loss: 0.3947\n",
      "Epoch [10/15], Step [1150/1700], Loss: 0.0579\n",
      "Epoch [10/15], Step [1200/1700], Loss: 0.0967\n",
      "Epoch [10/15], Step [1250/1700], Loss: 0.0910\n",
      "Epoch [10/15], Step [1300/1700], Loss: 0.2693\n",
      "Epoch [10/15], Step [1350/1700], Loss: 0.3001\n",
      "Epoch [10/15], Step [1400/1700], Loss: 0.3220\n",
      "Epoch [10/15], Step [1450/1700], Loss: 0.0660\n",
      "Epoch [10/15], Step [1500/1700], Loss: 0.1463\n",
      "Epoch [10/15], Step [1550/1700], Loss: 0.1195\n",
      "Epoch [10/15], Step [1600/1700], Loss: 0.3096\n",
      "Epoch [10/15], Step [1650/1700], Loss: 0.3067\n",
      "Epoch [10/15], Step [1700/1700], Loss: 2.6826\n",
      "Epoch 10/15 Summary | Train Loss: 0.2855 | Train Acc: 88.90%\n",
      "Epoch 10/15 Summary | Val Loss: 0.4727 | Val Acc: 81.11%\n",
      "Epoch [11/15], Step [50/1700], Loss: 0.1240\n",
      "Epoch [11/15], Step [100/1700], Loss: 0.0605\n",
      "Epoch [11/15], Step [150/1700], Loss: 0.1658\n",
      "Epoch [11/15], Step [200/1700], Loss: 0.2591\n",
      "Epoch [11/15], Step [250/1700], Loss: 0.2802\n",
      "Epoch [11/15], Step [300/1700], Loss: 0.2858\n",
      "Epoch [11/15], Step [350/1700], Loss: 0.2074\n",
      "Epoch [11/15], Step [400/1700], Loss: 0.1667\n",
      "Epoch [11/15], Step [450/1700], Loss: 0.2104\n",
      "Epoch [11/15], Step [500/1700], Loss: 0.0650\n",
      "Epoch [11/15], Step [550/1700], Loss: 0.8384\n",
      "Epoch [11/15], Step [600/1700], Loss: 0.3092\n",
      "Epoch [11/15], Step [650/1700], Loss: 0.1808\n",
      "Epoch [11/15], Step [700/1700], Loss: 0.3669\n",
      "Epoch [11/15], Step [750/1700], Loss: 0.2545\n",
      "Epoch [11/15], Step [800/1700], Loss: 0.2151\n",
      "Epoch [11/15], Step [850/1700], Loss: 0.2142\n",
      "Epoch [11/15], Step [900/1700], Loss: 0.3140\n",
      "Epoch [11/15], Step [950/1700], Loss: 0.3499\n",
      "Epoch [11/15], Step [1000/1700], Loss: 0.2592\n",
      "Epoch [11/15], Step [1050/1700], Loss: 0.2652\n",
      "Epoch [11/15], Step [1100/1700], Loss: 0.2134\n",
      "Epoch [11/15], Step [1150/1700], Loss: 0.1646\n",
      "Epoch [11/15], Step [1200/1700], Loss: 0.1878\n",
      "Epoch [11/15], Step [1250/1700], Loss: 0.1299\n",
      "Epoch [11/15], Step [1300/1700], Loss: 0.1130\n",
      "Epoch [11/15], Step [1350/1700], Loss: 0.5807\n",
      "Epoch [11/15], Step [1400/1700], Loss: 0.1501\n",
      "Epoch [11/15], Step [1450/1700], Loss: 0.1338\n",
      "Epoch [11/15], Step [1500/1700], Loss: 0.4282\n",
      "Epoch [11/15], Step [1550/1700], Loss: 0.3072\n",
      "Epoch [11/15], Step [1600/1700], Loss: 0.2960\n",
      "Epoch [11/15], Step [1650/1700], Loss: 0.1005\n",
      "Epoch [11/15], Step [1700/1700], Loss: 0.8093\n",
      "Epoch 11/15 Summary | Train Loss: 0.2623 | Train Acc: 89.69%\n",
      "Epoch 11/15 Summary | Val Loss: 0.4141 | Val Acc: 83.39%\n",
      "Epoch [12/15], Step [50/1700], Loss: 0.2175\n",
      "Epoch [12/15], Step [100/1700], Loss: 0.1615\n",
      "Epoch [12/15], Step [150/1700], Loss: 0.4116\n",
      "Epoch [12/15], Step [200/1700], Loss: 0.2071\n",
      "Epoch [12/15], Step [250/1700], Loss: 0.5355\n",
      "Epoch [12/15], Step [300/1700], Loss: 0.2239\n",
      "Epoch [12/15], Step [350/1700], Loss: 0.1638\n",
      "Epoch [12/15], Step [400/1700], Loss: 0.1414\n",
      "Epoch [12/15], Step [450/1700], Loss: 0.1733\n",
      "Epoch [12/15], Step [500/1700], Loss: 0.0806\n",
      "Epoch [12/15], Step [550/1700], Loss: 0.5964\n",
      "Epoch [12/15], Step [600/1700], Loss: 0.3106\n",
      "Epoch [12/15], Step [650/1700], Loss: 0.0749\n",
      "Epoch [12/15], Step [700/1700], Loss: 0.1926\n",
      "Epoch [12/15], Step [750/1700], Loss: 0.1548\n",
      "Epoch [12/15], Step [800/1700], Loss: 0.0795\n",
      "Epoch [12/15], Step [850/1700], Loss: 0.1721\n",
      "Epoch [12/15], Step [900/1700], Loss: 0.2037\n",
      "Epoch [12/15], Step [950/1700], Loss: 0.4688\n",
      "Epoch [12/15], Step [1000/1700], Loss: 0.3839\n",
      "Epoch [12/15], Step [1050/1700], Loss: 0.3162\n",
      "Epoch [12/15], Step [1100/1700], Loss: 0.0487\n",
      "Epoch [12/15], Step [1150/1700], Loss: 0.1584\n",
      "Epoch [12/15], Step [1200/1700], Loss: 0.3125\n",
      "Epoch [12/15], Step [1250/1700], Loss: 0.3682\n",
      "Epoch [12/15], Step [1300/1700], Loss: 0.2908\n",
      "Epoch [12/15], Step [1350/1700], Loss: 0.2475\n",
      "Epoch [12/15], Step [1400/1700], Loss: 0.2558\n",
      "Epoch [12/15], Step [1450/1700], Loss: 0.3339\n",
      "Epoch [12/15], Step [1500/1700], Loss: 0.1661\n",
      "Epoch [12/15], Step [1550/1700], Loss: 0.1418\n",
      "Epoch [12/15], Step [1600/1700], Loss: 0.2942\n",
      "Epoch [12/15], Step [1650/1700], Loss: 0.2315\n",
      "Epoch [12/15], Step [1700/1700], Loss: 0.0815\n",
      "Epoch 12/15 Summary | Train Loss: 0.2457 | Train Acc: 90.48%\n",
      "Epoch 12/15 Summary | Val Loss: 0.4348 | Val Acc: 83.23%\n",
      "Epoch [13/15], Step [50/1700], Loss: 0.1323\n",
      "Epoch [13/15], Step [100/1700], Loss: 0.4488\n",
      "Epoch [13/15], Step [150/1700], Loss: 0.1791\n",
      "Epoch [13/15], Step [200/1700], Loss: 0.2564\n",
      "Epoch [13/15], Step [250/1700], Loss: 0.3379\n",
      "Epoch [13/15], Step [300/1700], Loss: 0.4007\n",
      "Epoch [13/15], Step [350/1700], Loss: 0.0601\n",
      "Epoch [13/15], Step [400/1700], Loss: 0.5607\n",
      "Epoch [13/15], Step [450/1700], Loss: 0.2019\n",
      "Epoch [13/15], Step [500/1700], Loss: 0.0944\n",
      "Epoch [13/15], Step [550/1700], Loss: 0.1036\n",
      "Epoch [13/15], Step [600/1700], Loss: 0.3831\n",
      "Epoch [13/15], Step [650/1700], Loss: 0.1645\n",
      "Epoch [13/15], Step [700/1700], Loss: 0.7385\n",
      "Epoch [13/15], Step [750/1700], Loss: 0.2691\n",
      "Epoch [13/15], Step [800/1700], Loss: 0.0755\n",
      "Epoch [13/15], Step [850/1700], Loss: 0.0897\n",
      "Epoch [13/15], Step [900/1700], Loss: 0.1991\n",
      "Epoch [13/15], Step [950/1700], Loss: 0.1057\n",
      "Epoch [13/15], Step [1000/1700], Loss: 0.2547\n",
      "Epoch [13/15], Step [1050/1700], Loss: 0.0979\n",
      "Epoch [13/15], Step [1100/1700], Loss: 0.2220\n",
      "Epoch [13/15], Step [1150/1700], Loss: 0.4908\n",
      "Epoch [13/15], Step [1200/1700], Loss: 0.3668\n",
      "Epoch [13/15], Step [1250/1700], Loss: 0.6106\n",
      "Epoch [13/15], Step [1300/1700], Loss: 0.0926\n",
      "Epoch [13/15], Step [1350/1700], Loss: 0.1762\n",
      "Epoch [13/15], Step [1400/1700], Loss: 0.2031\n",
      "Epoch [13/15], Step [1450/1700], Loss: 0.2871\n",
      "Epoch [13/15], Step [1500/1700], Loss: 0.1858\n",
      "Epoch [13/15], Step [1550/1700], Loss: 0.2872\n",
      "Epoch [13/15], Step [1600/1700], Loss: 0.1364\n",
      "Epoch [13/15], Step [1650/1700], Loss: 0.2383\n",
      "Epoch [13/15], Step [1700/1700], Loss: 0.6115\n",
      "Epoch 13/15 Summary | Train Loss: 0.2311 | Train Acc: 91.21%\n",
      "Epoch 13/15 Summary | Val Loss: 0.4614 | Val Acc: 82.89%\n",
      "Epoch [14/15], Step [50/1700], Loss: 0.1947\n",
      "Epoch [14/15], Step [100/1700], Loss: 0.4099\n",
      "Epoch [14/15], Step [150/1700], Loss: 0.1285\n",
      "Epoch [14/15], Step [200/1700], Loss: 0.0947\n",
      "Epoch [14/15], Step [250/1700], Loss: 0.2283\n",
      "Epoch [14/15], Step [300/1700], Loss: 0.1146\n",
      "Epoch [14/15], Step [350/1700], Loss: 0.2682\n",
      "Epoch [14/15], Step [400/1700], Loss: 0.1869\n",
      "Epoch [14/15], Step [450/1700], Loss: 0.0654\n",
      "Epoch [14/15], Step [500/1700], Loss: 0.1181\n",
      "Epoch [14/15], Step [550/1700], Loss: 0.2473\n",
      "Epoch [14/15], Step [600/1700], Loss: 0.0575\n",
      "Epoch [14/15], Step [650/1700], Loss: 0.0973\n",
      "Epoch [14/15], Step [700/1700], Loss: 0.2045\n",
      "Epoch [14/15], Step [750/1700], Loss: 0.0916\n",
      "Epoch [14/15], Step [800/1700], Loss: 0.0800\n",
      "Epoch [14/15], Step [850/1700], Loss: 0.1828\n",
      "Epoch [14/15], Step [900/1700], Loss: 0.1299\n",
      "Epoch [14/15], Step [950/1700], Loss: 0.2795\n",
      "Epoch [14/15], Step [1000/1700], Loss: 0.0526\n",
      "Epoch [14/15], Step [1050/1700], Loss: 0.0831\n",
      "Epoch [14/15], Step [1100/1700], Loss: 0.2245\n",
      "Epoch [14/15], Step [1150/1700], Loss: 0.0916\n",
      "Epoch [14/15], Step [1200/1700], Loss: 0.4097\n",
      "Epoch [14/15], Step [1250/1700], Loss: 0.0368\n",
      "Epoch [14/15], Step [1300/1700], Loss: 0.1007\n",
      "Epoch [14/15], Step [1350/1700], Loss: 0.0158\n",
      "Epoch [14/15], Step [1400/1700], Loss: 0.4508\n",
      "Epoch [14/15], Step [1450/1700], Loss: 0.0687\n",
      "Epoch [14/15], Step [1500/1700], Loss: 0.1069\n",
      "Epoch [14/15], Step [1550/1700], Loss: 0.1154\n",
      "Epoch [14/15], Step [1600/1700], Loss: 0.1880\n",
      "Epoch [14/15], Step [1650/1700], Loss: 0.1597\n",
      "Epoch [14/15], Step [1700/1700], Loss: 0.9618\n",
      "Epoch 14/15 Summary | Train Loss: 0.2125 | Train Acc: 91.84%\n",
      "Epoch 14/15 Summary | Val Loss: 0.4537 | Val Acc: 83.10%\n",
      "Epoch [15/15], Step [50/1700], Loss: 0.0928\n",
      "Epoch [15/15], Step [100/1700], Loss: 0.0163\n",
      "Epoch [15/15], Step [150/1700], Loss: 0.0841\n",
      "Epoch [15/15], Step [200/1700], Loss: 0.0704\n",
      "Epoch [15/15], Step [250/1700], Loss: 0.0524\n",
      "Epoch [15/15], Step [300/1700], Loss: 0.2815\n",
      "Epoch [15/15], Step [350/1700], Loss: 0.1758\n",
      "Epoch [15/15], Step [400/1700], Loss: 0.1280\n",
      "Epoch [15/15], Step [450/1700], Loss: 0.2124\n",
      "Epoch [15/15], Step [500/1700], Loss: 0.0277\n",
      "Epoch [15/15], Step [550/1700], Loss: 0.1887\n",
      "Epoch [15/15], Step [600/1700], Loss: 0.0293\n",
      "Epoch [15/15], Step [650/1700], Loss: 0.3142\n",
      "Epoch [15/15], Step [700/1700], Loss: 0.3046\n",
      "Epoch [15/15], Step [750/1700], Loss: 0.3257\n",
      "Epoch [15/15], Step [800/1700], Loss: 0.1887\n",
      "Epoch [15/15], Step [850/1700], Loss: 0.4482\n",
      "Epoch [15/15], Step [900/1700], Loss: 0.2073\n",
      "Epoch [15/15], Step [950/1700], Loss: 0.0598\n",
      "Epoch [15/15], Step [1000/1700], Loss: 0.2622\n",
      "Epoch [15/15], Step [1050/1700], Loss: 0.0548\n",
      "Epoch [15/15], Step [1100/1700], Loss: 0.2926\n",
      "Epoch [15/15], Step [1150/1700], Loss: 0.4780\n",
      "Epoch [15/15], Step [1200/1700], Loss: 0.1731\n",
      "Epoch [15/15], Step [1250/1700], Loss: 0.0957\n",
      "Epoch [15/15], Step [1300/1700], Loss: 0.1526\n",
      "Epoch [15/15], Step [1350/1700], Loss: 0.2027\n",
      "Epoch [15/15], Step [1400/1700], Loss: 0.3055\n",
      "Epoch [15/15], Step [1450/1700], Loss: 0.3366\n",
      "Epoch [15/15], Step [1500/1700], Loss: 0.3399\n",
      "Epoch [15/15], Step [1550/1700], Loss: 0.3286\n",
      "Epoch [15/15], Step [1600/1700], Loss: 0.1950\n",
      "Epoch [15/15], Step [1650/1700], Loss: 0.4863\n",
      "Epoch [15/15], Step [1700/1700], Loss: 1.8197\n",
      "Epoch 15/15 Summary | Train Loss: 0.2072 | Train Acc: 92.14%\n",
      "Epoch 15/15 Summary | Val Loss: 0.4267 | Val Acc: 83.80%\n",
      "\n",
      "--- Starting Phase 2: Fine-tuning ---\n",
      "Unfroze 75 parameter groups in later MobileNetV2 layers.\n",
      "Unfroze 144 parameter groups in DeiT blocks.\n",
      "Created new optimizer for fine-tuning with LR: 1e-05\n",
      "\n",
      "--- Trainable Parameters After Unfreezing ---\n",
      "cls_token\n",
      "pos_embed\n",
      "mobilenet_features.10.conv.0.0.weight\n",
      "mobilenet_features.10.conv.0.1.weight\n",
      "mobilenet_features.10.conv.0.1.bias\n",
      "mobilenet_features.10.conv.1.0.weight\n",
      "mobilenet_features.10.conv.1.1.weight\n",
      "mobilenet_features.10.conv.1.1.bias\n",
      "mobilenet_features.10.conv.2.weight\n",
      "mobilenet_features.10.conv.3.weight\n",
      "mobilenet_features.10.conv.3.bias\n",
      "mobilenet_features.11.conv.0.0.weight\n",
      "mobilenet_features.11.conv.0.1.weight\n",
      "mobilenet_features.11.conv.0.1.bias\n",
      "mobilenet_features.11.conv.1.0.weight\n",
      "mobilenet_features.11.conv.1.1.weight\n",
      "mobilenet_features.11.conv.1.1.bias\n",
      "mobilenet_features.11.conv.2.weight\n",
      "mobilenet_features.11.conv.3.weight\n",
      "mobilenet_features.11.conv.3.bias\n",
      "mobilenet_features.12.conv.0.0.weight\n",
      "mobilenet_features.12.conv.0.1.weight\n",
      "mobilenet_features.12.conv.0.1.bias\n",
      "mobilenet_features.12.conv.1.0.weight\n",
      "mobilenet_features.12.conv.1.1.weight\n",
      "mobilenet_features.12.conv.1.1.bias\n",
      "mobilenet_features.12.conv.2.weight\n",
      "mobilenet_features.12.conv.3.weight\n",
      "mobilenet_features.12.conv.3.bias\n",
      "mobilenet_features.13.conv.0.0.weight\n",
      "mobilenet_features.13.conv.0.1.weight\n",
      "mobilenet_features.13.conv.0.1.bias\n",
      "mobilenet_features.13.conv.1.0.weight\n",
      "mobilenet_features.13.conv.1.1.weight\n",
      "mobilenet_features.13.conv.1.1.bias\n",
      "mobilenet_features.13.conv.2.weight\n",
      "mobilenet_features.13.conv.3.weight\n",
      "mobilenet_features.13.conv.3.bias\n",
      "mobilenet_features.14.conv.0.0.weight\n",
      "mobilenet_features.14.conv.0.1.weight\n",
      "mobilenet_features.14.conv.0.1.bias\n",
      "mobilenet_features.14.conv.1.0.weight\n",
      "mobilenet_features.14.conv.1.1.weight\n",
      "mobilenet_features.14.conv.1.1.bias\n",
      "mobilenet_features.14.conv.2.weight\n",
      "mobilenet_features.14.conv.3.weight\n",
      "mobilenet_features.14.conv.3.bias\n",
      "mobilenet_features.15.conv.0.0.weight\n",
      "mobilenet_features.15.conv.0.1.weight\n",
      "mobilenet_features.15.conv.0.1.bias\n",
      "mobilenet_features.15.conv.1.0.weight\n",
      "mobilenet_features.15.conv.1.1.weight\n",
      "mobilenet_features.15.conv.1.1.bias\n",
      "mobilenet_features.15.conv.2.weight\n",
      "mobilenet_features.15.conv.3.weight\n",
      "mobilenet_features.15.conv.3.bias\n",
      "mobilenet_features.16.conv.0.0.weight\n",
      "mobilenet_features.16.conv.0.1.weight\n",
      "mobilenet_features.16.conv.0.1.bias\n",
      "mobilenet_features.16.conv.1.0.weight\n",
      "mobilenet_features.16.conv.1.1.weight\n",
      "mobilenet_features.16.conv.1.1.bias\n",
      "mobilenet_features.16.conv.2.weight\n",
      "mobilenet_features.16.conv.3.weight\n",
      "mobilenet_features.16.conv.3.bias\n",
      "mobilenet_features.17.conv.0.0.weight\n",
      "mobilenet_features.17.conv.0.1.weight\n",
      "mobilenet_features.17.conv.0.1.bias\n",
      "mobilenet_features.17.conv.1.0.weight\n",
      "mobilenet_features.17.conv.1.1.weight\n",
      "mobilenet_features.17.conv.1.1.bias\n",
      "mobilenet_features.17.conv.2.weight\n",
      "mobilenet_features.17.conv.3.weight\n",
      "mobilenet_features.17.conv.3.bias\n",
      "mobilenet_features.18.0.weight\n",
      "mobilenet_features.18.1.weight\n",
      "mobilenet_features.18.1.bias\n",
      "projection.weight\n",
      "projection.bias\n",
      "deit_blocks.0.norm1.weight\n",
      "deit_blocks.0.norm1.bias\n",
      "deit_blocks.0.attn.qkv.weight\n",
      "deit_blocks.0.attn.qkv.bias\n",
      "deit_blocks.0.attn.proj.weight\n",
      "deit_blocks.0.attn.proj.bias\n",
      "deit_blocks.0.norm2.weight\n",
      "deit_blocks.0.norm2.bias\n",
      "deit_blocks.0.mlp.fc1.weight\n",
      "deit_blocks.0.mlp.fc1.bias\n",
      "deit_blocks.0.mlp.fc2.weight\n",
      "deit_blocks.0.mlp.fc2.bias\n",
      "deit_blocks.1.norm1.weight\n",
      "deit_blocks.1.norm1.bias\n",
      "deit_blocks.1.attn.qkv.weight\n",
      "deit_blocks.1.attn.qkv.bias\n",
      "deit_blocks.1.attn.proj.weight\n",
      "deit_blocks.1.attn.proj.bias\n",
      "deit_blocks.1.norm2.weight\n",
      "deit_blocks.1.norm2.bias\n",
      "deit_blocks.1.mlp.fc1.weight\n",
      "deit_blocks.1.mlp.fc1.bias\n",
      "deit_blocks.1.mlp.fc2.weight\n",
      "deit_blocks.1.mlp.fc2.bias\n",
      "deit_blocks.2.norm1.weight\n",
      "deit_blocks.2.norm1.bias\n",
      "deit_blocks.2.attn.qkv.weight\n",
      "deit_blocks.2.attn.qkv.bias\n",
      "deit_blocks.2.attn.proj.weight\n",
      "deit_blocks.2.attn.proj.bias\n",
      "deit_blocks.2.norm2.weight\n",
      "deit_blocks.2.norm2.bias\n",
      "deit_blocks.2.mlp.fc1.weight\n",
      "deit_blocks.2.mlp.fc1.bias\n",
      "deit_blocks.2.mlp.fc2.weight\n",
      "deit_blocks.2.mlp.fc2.bias\n",
      "deit_blocks.3.norm1.weight\n",
      "deit_blocks.3.norm1.bias\n",
      "deit_blocks.3.attn.qkv.weight\n",
      "deit_blocks.3.attn.qkv.bias\n",
      "deit_blocks.3.attn.proj.weight\n",
      "deit_blocks.3.attn.proj.bias\n",
      "deit_blocks.3.norm2.weight\n",
      "deit_blocks.3.norm2.bias\n",
      "deit_blocks.3.mlp.fc1.weight\n",
      "deit_blocks.3.mlp.fc1.bias\n",
      "deit_blocks.3.mlp.fc2.weight\n",
      "deit_blocks.3.mlp.fc2.bias\n",
      "deit_blocks.4.norm1.weight\n",
      "deit_blocks.4.norm1.bias\n",
      "deit_blocks.4.attn.qkv.weight\n",
      "deit_blocks.4.attn.qkv.bias\n",
      "deit_blocks.4.attn.proj.weight\n",
      "deit_blocks.4.attn.proj.bias\n",
      "deit_blocks.4.norm2.weight\n",
      "deit_blocks.4.norm2.bias\n",
      "deit_blocks.4.mlp.fc1.weight\n",
      "deit_blocks.4.mlp.fc1.bias\n",
      "deit_blocks.4.mlp.fc2.weight\n",
      "deit_blocks.4.mlp.fc2.bias\n",
      "deit_blocks.5.norm1.weight\n",
      "deit_blocks.5.norm1.bias\n",
      "deit_blocks.5.attn.qkv.weight\n",
      "deit_blocks.5.attn.qkv.bias\n",
      "deit_blocks.5.attn.proj.weight\n",
      "deit_blocks.5.attn.proj.bias\n",
      "deit_blocks.5.norm2.weight\n",
      "deit_blocks.5.norm2.bias\n",
      "deit_blocks.5.mlp.fc1.weight\n",
      "deit_blocks.5.mlp.fc1.bias\n",
      "deit_blocks.5.mlp.fc2.weight\n",
      "deit_blocks.5.mlp.fc2.bias\n",
      "deit_blocks.6.norm1.weight\n",
      "deit_blocks.6.norm1.bias\n",
      "deit_blocks.6.attn.qkv.weight\n",
      "deit_blocks.6.attn.qkv.bias\n",
      "deit_blocks.6.attn.proj.weight\n",
      "deit_blocks.6.attn.proj.bias\n",
      "deit_blocks.6.norm2.weight\n",
      "deit_blocks.6.norm2.bias\n",
      "deit_blocks.6.mlp.fc1.weight\n",
      "deit_blocks.6.mlp.fc1.bias\n",
      "deit_blocks.6.mlp.fc2.weight\n",
      "deit_blocks.6.mlp.fc2.bias\n",
      "deit_blocks.7.norm1.weight\n",
      "deit_blocks.7.norm1.bias\n",
      "deit_blocks.7.attn.qkv.weight\n",
      "deit_blocks.7.attn.qkv.bias\n",
      "deit_blocks.7.attn.proj.weight\n",
      "deit_blocks.7.attn.proj.bias\n",
      "deit_blocks.7.norm2.weight\n",
      "deit_blocks.7.norm2.bias\n",
      "deit_blocks.7.mlp.fc1.weight\n",
      "deit_blocks.7.mlp.fc1.bias\n",
      "deit_blocks.7.mlp.fc2.weight\n",
      "deit_blocks.7.mlp.fc2.bias\n",
      "deit_blocks.8.norm1.weight\n",
      "deit_blocks.8.norm1.bias\n",
      "deit_blocks.8.attn.qkv.weight\n",
      "deit_blocks.8.attn.qkv.bias\n",
      "deit_blocks.8.attn.proj.weight\n",
      "deit_blocks.8.attn.proj.bias\n",
      "deit_blocks.8.norm2.weight\n",
      "deit_blocks.8.norm2.bias\n",
      "deit_blocks.8.mlp.fc1.weight\n",
      "deit_blocks.8.mlp.fc1.bias\n",
      "deit_blocks.8.mlp.fc2.weight\n",
      "deit_blocks.8.mlp.fc2.bias\n",
      "deit_blocks.9.norm1.weight\n",
      "deit_blocks.9.norm1.bias\n",
      "deit_blocks.9.attn.qkv.weight\n",
      "deit_blocks.9.attn.qkv.bias\n",
      "deit_blocks.9.attn.proj.weight\n",
      "deit_blocks.9.attn.proj.bias\n",
      "deit_blocks.9.norm2.weight\n",
      "deit_blocks.9.norm2.bias\n",
      "deit_blocks.9.mlp.fc1.weight\n",
      "deit_blocks.9.mlp.fc1.bias\n",
      "deit_blocks.9.mlp.fc2.weight\n",
      "deit_blocks.9.mlp.fc2.bias\n",
      "deit_blocks.10.norm1.weight\n",
      "deit_blocks.10.norm1.bias\n",
      "deit_blocks.10.attn.qkv.weight\n",
      "deit_blocks.10.attn.qkv.bias\n",
      "deit_blocks.10.attn.proj.weight\n",
      "deit_blocks.10.attn.proj.bias\n",
      "deit_blocks.10.norm2.weight\n",
      "deit_blocks.10.norm2.bias\n",
      "deit_blocks.10.mlp.fc1.weight\n",
      "deit_blocks.10.mlp.fc1.bias\n",
      "deit_blocks.10.mlp.fc2.weight\n",
      "deit_blocks.10.mlp.fc2.bias\n",
      "deit_blocks.11.norm1.weight\n",
      "deit_blocks.11.norm1.bias\n",
      "deit_blocks.11.attn.qkv.weight\n",
      "deit_blocks.11.attn.qkv.bias\n",
      "deit_blocks.11.attn.proj.weight\n",
      "deit_blocks.11.attn.proj.bias\n",
      "deit_blocks.11.norm2.weight\n",
      "deit_blocks.11.norm2.bias\n",
      "deit_blocks.11.mlp.fc1.weight\n",
      "deit_blocks.11.mlp.fc1.bias\n",
      "deit_blocks.11.mlp.fc2.weight\n",
      "deit_blocks.11.mlp.fc2.bias\n",
      "deit_norm.weight\n",
      "deit_norm.bias\n",
      "classifier.weight\n",
      "classifier.bias\n",
      "\n",
      "Training finished.\n",
      "\n",
      "--- Generating Final Report and Saving Results ---\n",
      "Model weights saved to training_results/mobilenet_deit_hybrid_final_weights.pth\n",
      "Loss plot saved to training_results/mobilenet_deit_hybrid_loss_plot.png\n",
      "Accuracy plot saved to training_results/mobilenet_deit_hybrid_accuracy_plot.png\n",
      "\n",
      "Validation F1 Score (Macro): 0.8472\n",
      "Validation F1 Score (Weighted): 0.8366\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CN       0.98      0.98      0.98      1293\n",
      "        EMCI       0.83      0.79      0.81      1920\n",
      "        LMCI       0.78      0.72      0.75      1792\n",
      "          AD       0.80      0.91      0.85      1792\n",
      "\n",
      "    accuracy                           0.84      6797\n",
      "   macro avg       0.85      0.85      0.85      6797\n",
      "weighted avg       0.84      0.84      0.84      6797\n",
      "\n",
      "Confusion matrix plot saved to training_results/mobilenet_deit_hybrid_confusion_matrix.png\n",
      "All results saved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import timm\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import kagglehub\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "\n",
    "DEIT_MODEL_NAME = 'deit_small_patch16_224'\n",
    "DEIT_EMBED_DIM = 384\n",
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 16\n",
    "NUM_CLASSES = 4\n",
    "LEARNING_RATE = 1e-4\n",
    "LEARNING_RATE_FINETUNE = 1e-5\n",
    "EPOCHS = 15\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "RESULTS_DIR = \"training_results\"\n",
    "if not os.path.exists(RESULTS_DIR):\n",
    "    os.makedirs(RESULTS_DIR)\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Timm version: {timm.__version__}\")\n",
    "\n",
    "print(\"Downloading dataset...\")\n",
    "try:\n",
    "    data_dir = kagglehub.dataset_download(\"abdullahtauseef2003/adni-4c-alzheimers-mri-classification-dataset\")\n",
    "    print(f\"Dataset downloaded to: {data_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading dataset via kagglehub: {e}\")\n",
    "data_dir = \"./adni_data\"\n",
    "print(f\"Attempting to use local path: {data_dir}\")\n",
    "if not os.path.isdir(data_dir):\n",
    "    exit(f\"Error: Dataset directory not found at fallback path: {data_dir}\")\n",
    "\n",
    "diagnosis_mapping = { 0: 'CN', 1: 'EMCI', 2: 'LMCI', 3: 'AD' }\n",
    "dir_to_code = { 'CN': 0, 'EMCI': 1, 'LMCI': 2, 'AD': 3 }\n",
    "class_names = list(diagnosis_mapping.values())\n",
    "\n",
    "def get_image_paths_and_labels(base_data_dir):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    possible_image_dirs = [\n",
    "        os.path.join(base_data_dir, 'ADNI_4C_MRI_Classification_Dataset', 'AugmentedAlzheimerDataset'),\n",
    "        os.path.join(base_data_dir, 'AugmentedAlzheimerDataset'),\n",
    "        os.path.join(base_data_dir, 'ADNI_IMAGES')\n",
    "    ]\n",
    "    images_dir = None\n",
    "    print(\"--- Searching for Image Directory ---\")\n",
    "    for dir_path in possible_image_dirs:\n",
    "        print(f\"Checking: {dir_path}\")\n",
    "        if os.path.isdir(dir_path):\n",
    "            has_any_subdir = any(os.path.isdir(os.path.join(dir_path, class_name)) for class_name in dir_to_code)\n",
    "            if has_any_subdir:\n",
    "                images_dir = dir_path\n",
    "                print(f\"Found valid image directory: {images_dir}\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"  > Directory exists but lacks expected class subdirectories.\")\n",
    "        else:\n",
    "            print(f\"  > Directory does not exist.\")\n",
    "    print(\"--- Search Complete ---\")\n",
    "\n",
    "    if images_dir is None:\n",
    "        print(f\"\\nERROR: Could not find a valid image directory structure within '{base_data_dir}'\")\n",
    "        return [], []\n",
    "\n",
    "    print(f\"\\nUsing image directory: {images_dir}\")\n",
    "    for class_name, class_code in dir_to_code.items():\n",
    "        class_dir = os.path.join(images_dir, class_name)\n",
    "        if os.path.isdir(class_dir):\n",
    "            try:\n",
    "                files = [f for f in os.listdir(class_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "                print(f\"Found {len(files)} images in '{class_name}'\")\n",
    "                for file in files:\n",
    "                    image_paths.append(os.path.join(class_dir, file))\n",
    "                    labels.append(class_code)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading directory {class_dir}: {e}\")\n",
    "        else:\n",
    "            print(f\"Warning: Class directory '{class_dir}' not found. Skipping this class.\")\n",
    "\n",
    "    if not image_paths:\n",
    "        print(\"Warning: No image paths were collected. Check class subdirectories and file types.\")\n",
    "    return image_paths, labels\n",
    "\n",
    "print(\"\\nCollecting image paths and labels...\")\n",
    "image_paths, labels = get_image_paths_and_labels(data_dir)\n",
    "\n",
    "if not image_paths:\n",
    "    print(\"\\nNo images found. Please check the dataset path and structure.\")\n",
    "    exit()\n",
    "\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(f\"Total images found: {len(image_paths)}\")\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "print(\"\\nClass distribution:\")\n",
    "if len(unique) > 0:\n",
    "    for label_code, count in zip(unique, counts):\n",
    "        print(f\"Class {diagnosis_mapping.get(label_code, f'Unknown ({label_code})')}: {count} images\")\n",
    "else:\n",
    "    print(\"No labels found.\")\n",
    "\n",
    "class AlzheimerMRIDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {self.image_paths[idx]}: {e}\")\n",
    "            return None, None\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE[0], IMAGE_SIZE[1])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(image_paths, labels, test_size=0.2, stratify=labels, random_state=42)\n",
    "\n",
    "print(f\"\\nTraining set size: {len(X_train)}\")\n",
    "print(f\"Validation set size: {len(X_val)}\")\n",
    "\n",
    "train_dataset = AlzheimerMRIDataset(X_train, y_train, transform)\n",
    "val_dataset = AlzheimerMRIDataset(X_val, y_val, transform)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = list(filter(lambda x: x[0] is not None, batch))\n",
    "    if not batch: return torch.Tensor(), torch.Tensor()\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True, collate_fn=collate_fn)\n",
    "\n",
    "print(\"DataLoaders created.\")\n",
    "\n",
    "class MobileNet_DeiT_Hybrid(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES, deit_model_name=DEIT_MODEL_NAME, deit_embed_dim=DEIT_EMBED_DIM, freeze_backbone=True, freeze_deit=True):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.deit_embed_dim = deit_embed_dim\n",
    "        mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "        self.mobilenet_features = mobilenet.features\n",
    "        self.num_cnn_features = 1280\n",
    "        self.feature_map_size = 7\n",
    "\n",
    "        if freeze_backbone:\n",
    "            for param in self.mobilenet_features.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.projection = nn.Conv2d(self.num_cnn_features, self.deit_embed_dim, kernel_size=1)\n",
    "        deit_full = timm.create_model(deit_model_name, pretrained=True)\n",
    "        self.deit_blocks = deit_full.blocks\n",
    "        self.deit_norm = deit_full.norm\n",
    "\n",
    "        if freeze_deit:\n",
    "            for param in self.deit_blocks.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.num_patches = self.feature_map_size * self.feature_map_size\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.deit_embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, self.deit_embed_dim))\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=.02)\n",
    "\n",
    "        self.classifier = nn.Linear(self.deit_embed_dim, self.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mobilenet_features(x)\n",
    "        x = self.projection(x)\n",
    "        B, D, H, W = x.shape\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.deit_blocks(x)\n",
    "        x = self.deit_norm(x)\n",
    "        cls_output = x[:, 0]\n",
    "        out = self.classifier(cls_output)\n",
    "        return out\n",
    "\n",
    "def save_and_report_results(model, val_loader, device, history, class_names, output_prefix):\n",
    "    print(\"\\n--- Generating Final Report and Saving Results ---\")\n",
    "    weights_path = os.path.join(RESULTS_DIR, f\"{output_prefix}_final_weights.pth\")\n",
    "    torch.save(model.state_dict(), weights_path)\n",
    "    print(f\"Model weights saved to {weights_path}\")\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history['train_loss'], label='Training Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    loss_plot_path = os.path.join(RESULTS_DIR, f\"{output_prefix}_loss_plot.png\")\n",
    "    plt.savefig(loss_plot_path)\n",
    "    plt.close()\n",
    "    print(f\"Loss plot saved to {loss_plot_path}\")\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history['train_acc'], label='Training Accuracy')\n",
    "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    acc_plot_path = os.path.join(RESULTS_DIR, f\"{output_prefix}_accuracy_plot.png\")\n",
    "    plt.savefig(acc_plot_path)\n",
    "    plt.close()\n",
    "    print(f\"Accuracy plot saved to {acc_plot_path}\")\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            if images.numel() == 0: continue\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    if not all_labels or not all_preds:\n",
    "        print(\"Could not generate F1 score or confusion matrix due to empty validation predictions.\")\n",
    "        return\n",
    "\n",
    "    f1_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    f1_weighted = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    print(f\"\\nValidation F1 Score (Macro): {f1_macro:.4f}\")\n",
    "    print(f\"Validation F1 Score (Weighted): {f1_weighted:._\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6810406,
     "sourceId": 10948930,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
